{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "\n",
    "#########################Load the experiment result file#########################################\n",
    "# directory of the experiment result files\n",
    "outdir = '../../EPFLAerosenseData/EPFLAerosenseData/'\n",
    "#csv of experiments\n",
    "experiment_result = \"_DOE_Aerosense_Dynamic_Experiments_EPFL.xlsx\"\n",
    "\n",
    "attack0 = pd.read_excel(outdir+experiment_result, sheet_name='0_deg_angle_attack')\n",
    "attack8 = pd.read_excel(outdir+experiment_result, sheet_name='8_deg_angle_attack')\n",
    "\n",
    "relevant_attribute0 = [\"Experiment Number\", \"Zeroing\", \"Heaving frequency in [Hz],  from motor excitations\", \"Wind speed [m/s]\", \n",
    "                        \"Crack length [mm]\",  \"Concentrated mass [yes = 1, no = 0]\" ]\n",
    "\n",
    "relevant_attribute8 = [\"Experiment Number\",\"Zeroing\", \"Heaving frequency in [Hz],  from motor excitations\", \"Wind speed [m/s]\", \n",
    "                        \"Crack length [mm]\",  \"Concentrated mass\"  ]\n",
    "\n",
    "data_attack0 = attack0[relevant_attribute0]\n",
    "data_attack8 = attack8[relevant_attribute8]\n",
    "\n",
    "\n",
    "#######################  Load the time series data ###################################################\n",
    "\n",
    "dict_attack0 = data_attack0.to_dict(orient = \"records\")\n",
    "\n",
    "for i in dict_attack0:\n",
    "    exp_num = i[\"Experiment Number\"]    \n",
    "    \n",
    "    filename_pre = \"aoa_0deg_Exp_\"\n",
    "    if exp_num < 10:\n",
    "        filename_num = \"00\" + str(exp_num)\n",
    "    elif exp_num < 100:\n",
    "        filename_num = \"0\" + str(exp_num)\n",
    "    else:\n",
    "        filename_num = str(exp_num)        \n",
    "    filename_sub = \"_aerosense\"\n",
    "    complete_name = filename_pre + filename_num + filename_sub\n",
    "    complete_path = outdir +\"aerosense_aerodynamic_data/\" +  \"aoa_0deg/\" +complete_name + \"/\" + \"1_baros_p.csv\" \n",
    "    \n",
    "    csv_data = pd.read_csv(complete_path,header=None,skiprows=2)\n",
    "    i[\"csv_data\"] = csv_data.iloc[:-1,1:-1] #first column of time is not useful, last row maybe incomplete, las column is nan, drop them\n",
    "    i[\"csv_data\"] = i[\"csv_data\"].drop(columns=[23,37])    \n",
    "\n",
    "\n",
    "#######################  Dont use the zeroing experiments  ###################################################\n",
    "\n",
    "dic_attack0_filtered  = []\n",
    "dic_attack0_filtered_group_by = {}\n",
    "for i in dict_attack0:\n",
    "    if i[\"Zeroing\"] != \"zeroing\" and i[\"Wind speed [m/s]\"]!= 0: # only the expriments with heaving and wind\n",
    "        exp_num = i[\"Experiment Number\"]\n",
    "        #group by every 3 experiments\n",
    "        #every first 2 used for training and the 3rd for testing\n",
    "        if exp_num-1 not in dic_attack0_filtered_group_by:\n",
    "            dic_attack0_filtered_group_by[exp_num] = (exp_num, \"training\")\n",
    "        elif exp_num-1 in dic_attack0_filtered_group_by and exp_num-2  not in dic_attack0_filtered_group_by:\n",
    "            dic_attack0_filtered_group_by[exp_num] = (exp_num - 1,\"training\")\n",
    "        elif exp_num-1 in dic_attack0_filtered_group_by and exp_num-2  in dic_attack0_filtered_group_by:\n",
    "            dic_attack0_filtered_group_by[exp_num] = (exp_num -2, \"testing\")\n",
    "\n",
    "\n",
    "\n",
    "############################# Signal Windowing #############################################\n",
    "signal_windowing = []\n",
    "for i_key, i_value in dic_attack0_filtered_group_by.items():\n",
    "    exp_ind = i_key - 1 #to get the experiment  number\n",
    "    exp_i = dict_attack0[exp_ind] # get the whole dictionary of the corresponding experiment\n",
    "    df_csv_data = exp_i[\"csv_data\"]\n",
    "    df_csv_data = df_csv_data.iloc[2000:] # Drop the first 2000 rows    \n",
    "    num_rows_per_block = 2000# Calculate the number of rows in each of the 6 blocks\n",
    "    num_blocks = 6  #len(df) // num_rows_per_block\n",
    "    # Split the DataFrame into 6 blocks of 2000 rows each\n",
    "    total_number_rows = num_rows_per_block * num_blocks\n",
    "    start_block = [ i  for i in range(0,10001,1000)] #starting position of each block: [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "    end_block = [i + 2000 for i in start_block]\n",
    "    start_end_ind = [ (i,j) for i,j in zip(start_block, end_block)]\n",
    "    blocks = [df_csv_data.iloc[ind[0] : ind[1] ] for ind in start_end_ind]\n",
    "    for block_ind, block in enumerate(blocks):\n",
    "        window = {key: value for key, value in exp_i.items() if key != \"csv_data\"}         \n",
    "        window[\"block_ind\"] = block_ind\n",
    "        window[\"exp_group\"] = i_value[0]\n",
    "        window[\"training_or_testing\"] = i_value[1]\n",
    "        window[\"block\"] = block        \n",
    "        signal_windowing.append(window)\n",
    "\n",
    "windowing_list = []\n",
    "for i in signal_windowing:\n",
    "    windowing_list.append({ i_key: i_value for i_key, i_value in i.items() if i_key != \"block\"})\n",
    "windowing_df = pd.DataFrame(windowing_list)\n",
    "\n",
    "time_series_dict = {}\n",
    "n_total = len(signal_windowing)\n",
    "for i in range(n_total):\n",
    "    time_series_dict[i] = signal_windowing[i][\"block\"].to_dict(orient=\"series\")\n",
    "time_series_df = pd.DataFrame(time_series_dict).T\n",
    "\n",
    "#Concentrated mass changed to class 5\n",
    "for i in range(windowing_df.shape[0]):\n",
    "    if windowing_df.iloc[i,5] == 1:\n",
    "        windowing_df.iloc[i,4] = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for index, row in time_series_df.iterrows():\n",
    "    experiment_data = []\n",
    "    # Iterate over each cell in the row\n",
    "    for cell in row:\n",
    "        # Convert the Pandas Series in each cell to a NumPy array and append to the experiment data\n",
    "        experiment_data.append(cell.to_numpy())\n",
    "    # Stack the sensor data for each experiment and append to the processed data\n",
    "    processed_data.append(np.stack(experiment_data, axis=0))\n",
    "data = np.array(processed_data)\n",
    "\n",
    "#normalize the data\n",
    "data = (data - data.min()) / (data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 38, 2000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_20 (LSTM)              (None, 38, 64)            528640    \n",
      "                                                                 \n",
      " lstm_21 (LSTM)              (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 569984 (2.17 MB)\n",
      "Trainable params: 569984 (2.17 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Encoder with LSTM\n",
    "latent_dim = 128  # Size of the latent vector\n",
    "\n",
    "encoder_inputs = layers.Input(shape=(38, 2000))\n",
    "x = layers.LSTM(256, return_sequences=True)(encoder_inputs)\n",
    "x = layers.LSTM(128)(x)\n",
    "latent_vector = layers.Dense(latent_dim, activation='relu')(x)\n",
    "encoder = Model(encoder_inputs, latent_vector, name=\"encoder\")\n",
    "\n",
    "# Decoder with LSTM\n",
    "decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.RepeatVector(38)(decoder_inputs)  # Adjusting the input shape for LSTM\n",
    "x = layers.LSTM(128, return_sequences=True)(x)\n",
    "x = layers.LSTM(256, return_sequences=True)(x)\n",
    "decoder_outputs = layers.TimeDistributed(layers.Dense(2000, activation=\"sigmoid\"))(x)\n",
    "decoder = Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# Autoencoder Model\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder = Autoencoder(encoder, decoder)\n",
    "\n",
    "# Example of compiling the model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 128 and 2000 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_4/dense_14/BiasAdd, IteratorGetNext:1)' with input shapes: [?,128], [?,38,2000].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\DSLCodeBase\\FeatureExtraction\\LSTMFeatureExtractorOverlapping.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/DSLCodeBase/FeatureExtraction/LSTMFeatureExtractorOverlapping.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mfit(data, data, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/DSLCodeBase/FeatureExtraction/LSTMFeatureExtractorOverlapping.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Create a new model to output features from the penultimate layer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/DSLCodeBase/FeatureExtraction/LSTMFeatureExtractorOverlapping.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m feature_model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39moutput)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqrhkh5h8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 128 and 2000 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_4/dense_14/BiasAdd, IteratorGetNext:1)' with input shapes: [?,128], [?,38,2000].\n"
     ]
    }
   ],
   "source": [
    "# Assume `x_train` is your training data\n",
    "autoencoder.fit(data, data, epochs=10, batch_size=32, verbose = 1)\n",
    "# To get the latent features for some data\n",
    "latent_features = encoder.predict(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 129s 5s/step\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoded)\n",
    "latent_features = encoder_model.predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 2000, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
