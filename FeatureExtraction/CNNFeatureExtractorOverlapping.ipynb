{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "\n",
    "#########################Load the experiment result file#########################################\n",
    "# directory of the experiment result files\n",
    "outdir = '../../EPFLAerosenseData/EPFLAerosenseData/'\n",
    "#csv of experiments\n",
    "experiment_result = \"_DOE_Aerosense_Dynamic_Experiments_EPFL.xlsx\"\n",
    "\n",
    "attack0 = pd.read_excel(outdir+experiment_result, sheet_name='0_deg_angle_attack')\n",
    "attack8 = pd.read_excel(outdir+experiment_result, sheet_name='8_deg_angle_attack')\n",
    "\n",
    "relevant_attribute0 = [\"Experiment Number\", \"Zeroing\", \"Heaving frequency in [Hz],  from motor excitations\", \"Wind speed [m/s]\", \n",
    "                        \"Crack length [mm]\",  \"Concentrated mass [yes = 1, no = 0]\" ]\n",
    "\n",
    "relevant_attribute8 = [\"Experiment Number\",\"Zeroing\", \"Heaving frequency in [Hz],  from motor excitations\", \"Wind speed [m/s]\", \n",
    "                        \"Crack length [mm]\",  \"Concentrated mass\"  ]\n",
    "\n",
    "data_attack0 = attack0[relevant_attribute0]\n",
    "data_attack8 = attack8[relevant_attribute8]\n",
    "\n",
    "\n",
    "#######################  Load the time series data ###################################################\n",
    "\n",
    "dict_attack0 = data_attack0.to_dict(orient = \"records\")\n",
    "\n",
    "for i in dict_attack0:\n",
    "    exp_num = i[\"Experiment Number\"]    \n",
    "    \n",
    "    filename_pre = \"aoa_0deg_Exp_\"\n",
    "    if exp_num < 10:\n",
    "        filename_num = \"00\" + str(exp_num)\n",
    "    elif exp_num < 100:\n",
    "        filename_num = \"0\" + str(exp_num)\n",
    "    else:\n",
    "        filename_num = str(exp_num)        \n",
    "    filename_sub = \"_aerosense\"\n",
    "    complete_name = filename_pre + filename_num + filename_sub\n",
    "    complete_path = outdir +\"aerosense_aerodynamic_data/\" +  \"aoa_0deg/\" +complete_name + \"/\" + \"1_baros_p.csv\" \n",
    "    \n",
    "    csv_data = pd.read_csv(complete_path,header=None,skiprows=2)\n",
    "    i[\"csv_data\"] = csv_data.iloc[:-1,1:-1] #first column of time is not useful, last row maybe incomplete, las column is nan, drop them\n",
    "    i[\"csv_data\"] = i[\"csv_data\"].drop(columns=[23,37])    \n",
    "\n",
    "\n",
    "#######################  Dont use the zeroing experiments  ###################################################\n",
    "\n",
    "dic_attack0_filtered  = []\n",
    "dic_attack0_filtered_group_by = {}\n",
    "for i in dict_attack0:\n",
    "    if i[\"Zeroing\"] != \"zeroing\" and i[\"Wind speed [m/s]\"]!= 0: # only the expriments with heaving and wind\n",
    "        exp_num = i[\"Experiment Number\"]\n",
    "        #group by every 3 experiments\n",
    "        #every first 2 used for training and the 3rd for testing\n",
    "        if exp_num-1 not in dic_attack0_filtered_group_by:\n",
    "            dic_attack0_filtered_group_by[exp_num] = (exp_num, \"training\")\n",
    "        elif exp_num-1 in dic_attack0_filtered_group_by and exp_num-2  not in dic_attack0_filtered_group_by:\n",
    "            dic_attack0_filtered_group_by[exp_num] = (exp_num - 1,\"training\")\n",
    "        elif exp_num-1 in dic_attack0_filtered_group_by and exp_num-2  in dic_attack0_filtered_group_by:\n",
    "            dic_attack0_filtered_group_by[exp_num] = (exp_num -2, \"testing\")\n",
    "\n",
    "\n",
    "\n",
    "############################# Signal Windowing #############################################\n",
    "signal_windowing = []\n",
    "for i_key, i_value in dic_attack0_filtered_group_by.items():\n",
    "    exp_ind = i_key - 1 #to get the experiment  number\n",
    "    exp_i = dict_attack0[exp_ind] # get the whole dictionary of the corresponding experiment\n",
    "    df_csv_data = exp_i[\"csv_data\"]\n",
    "    df_csv_data = df_csv_data.iloc[2000:] # Drop the first 2000 rows    \n",
    "    num_rows_per_block = 2000# Calculate the number of rows in each of the 6 blocks\n",
    "    num_blocks = 6  #len(df) // num_rows_per_block\n",
    "    # Split the DataFrame into 6 blocks of 2000 rows each\n",
    "    total_number_rows = num_rows_per_block * num_blocks\n",
    "    start_block = [ i  for i in range(0,10001,1000)] #starting position of each block: [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "    end_block = [i + 2000 for i in start_block]\n",
    "    start_end_ind = [ (i,j) for i,j in zip(start_block, end_block)]\n",
    "    blocks = [df_csv_data.iloc[ind[0] : ind[1] ] for ind in start_end_ind]\n",
    "    for block_ind, block in enumerate(blocks):\n",
    "        window = {key: value for key, value in exp_i.items() if key != \"csv_data\"}         \n",
    "        window[\"block_ind\"] = block_ind\n",
    "        window[\"exp_group\"] = i_value[0]\n",
    "        window[\"training_or_testing\"] = i_value[1]\n",
    "        window[\"block\"] = block        \n",
    "        signal_windowing.append(window)\n",
    "\n",
    "windowing_list = []\n",
    "for i in signal_windowing:\n",
    "    windowing_list.append({ i_key: i_value for i_key, i_value in i.items() if i_key != \"block\"})\n",
    "windowing_df = pd.DataFrame(windowing_list)\n",
    "\n",
    "time_series_dict = {}\n",
    "n_total = len(signal_windowing)\n",
    "for i in range(n_total):\n",
    "    time_series_dict[i] = signal_windowing[i][\"block\"].to_dict(orient=\"series\")\n",
    "time_series_df = pd.DataFrame(time_series_dict).T\n",
    "\n",
    "#Concentrated mass changed to class 5\n",
    "for i in range(windowing_df.shape[0]):\n",
    "    if windowing_df.iloc[i,5] == 1:\n",
    "        windowing_df.iloc[i,4] = 25\n",
    "\n",
    "processed_data = []\n",
    "for index, row in time_series_df.iterrows():\n",
    "    experiment_data = []\n",
    "    # Iterate over each cell in the row\n",
    "    for cell in row:\n",
    "        # Convert the Pandas Series in each cell to a NumPy array and append to the experiment data\n",
    "        experiment_data.append(cell.to_numpy())\n",
    "    # Stack the sensor data for each experiment and append to the processed data\n",
    "    processed_data.append(np.stack(experiment_data, axis=0))\n",
    "data = np.array(processed_data)\n",
    "\n",
    "#normalize the data\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your 792x38x2000 dataset\n",
    "data = np.random.rand(792, 38, 2000)  # Replace this with your actual data\n",
    "labels = np.random.rand(792, 1)  # Dummy labels, replace with your actual labels if available\n",
    "\n",
    "# Reshape data for CNN (adding channel dimension)\n",
    "data = data.reshape((-1, 38, 2000, 1))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(38, 2000, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Change the output units and activation based on your problem\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Extract features\n",
    "feature_model = tf.keras.models.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "features = feature_model.predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 41 and 38 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_3/conv2d_19/Sigmoid, IteratorGetNext:1)' with input shapes: [?,41,2001,1], [?,38,2000,1].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\DSLCodeBase\\FeatureExtraction\\CNNFeatureExtractorOverlapping.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/DSLCodeBase/FeatureExtraction/CNNFeatureExtractorOverlapping.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mfit(data_reshaped, data_reshaped, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7e_75t78.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 41 and 38 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_3/conv2d_19/Sigmoid, IteratorGetNext:1)' with input shapes: [?,41,2001,1], [?,38,2000,1].\n"
     ]
    }
   ],
   "source": [
    "model.fit(data_reshaped, data_reshaped, epochs=10, batch_size=32, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = models.Model(inputs=model.input, outputs=model.get_layer('encoded').output)\n",
    "features = encoder_model.predict(data_reshaped)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
